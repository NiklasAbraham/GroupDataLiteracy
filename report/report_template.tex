%!TEX root = report_template.tex

%%%%%%%% DATA LITERACY 2025 LATEX PROJECT TEMPLATE FILE %%%%%%%%%%%%%%%%%
%%% Based on the 2025 ICML template, available at https://icml.cc/Conferences/2025/AuthorInstructions %%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{makecell} % for multi-line table cells
\usepackage{amsmath}   % For \text{}, align environments, better math spacing
\usepackage{amssymb}   % For \mathbb, extra symbols (optional but common)

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, calc, fit}
% Corporate Design of the University of Tübingen
% Primary Colors
\definecolor{TUred}{RGB}{165,30,55}
\definecolor{TUgold}{RGB}{180,160,105}
\definecolor{TUdark}{RGB}{50,65,75}
\definecolor{TUgray}{RGB}{175,179,183}

% Secondary Colors
\definecolor{TUdarkblue}{RGB}{65,90,140}
\definecolor{TUblue}{RGB}{0,105,170}
\definecolor{TUlightblue}{RGB}{80,170,200}
\definecolor{TUlightgreen}{RGB}{130,185,160}
\definecolor{TUgreen}{RGB}{125,165,75}
\definecolor{TUdarkgreen}{RGB}{50,110,30}
\definecolor{TUocre}{RGB}{200,80,60}
\definecolor{TUviolet}{RGB}{175,110,150}
\definecolor{TUmauve}{RGB}{180,160,150}
\definecolor{TUbeige}{RGB}{215,180,105}
\definecolor{TUorange}{RGB}{210,150,0}
\definecolor{TUbrown}{RGB}{145,105,70}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Project Report Template for Data Literacy 2025}

\begin{document}

\twocolumn[
% \icmltitle{The Geometry of Cinema: Quantifying 75 Years of Cultural Drift}
\icmltitle{Data Literacy 2025 Project Report}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ansel Cheung}{equal,first}
\icmlauthor{Alessio Villa}{equal,second}
\icmlauthor{Bartol Markovinović}{equal,third}
\icmlauthor{Martín López de Ipi\~na}{equal,fourth}
\icmlauthor{Niklas Abraham}{equal,fifth}
\end{icmlauthorlist}

% fill in your matrikelnummer, email address, degree, for each group member
\icmlaffiliation{first}{Matrikelnummer 7274374, MSc Machine Learning}
\icmlaffiliation{second}{Matrikelnummer 7306912, MSc Computer Science}
\icmlaffiliation{third}{Matrikelnummer 7324790, MSc Machine Learning}
\icmlaffiliation{fourth}{Matrikelnummer 7293076, MSc Machine Learning}
\icmlaffiliation{fifth}{Matrikelnummer 7307188, MSc Machine Learning}

% put your email addresses here. You can use initials to save space, 
% e.g. if you are called Max Mustermann, you can use \icmlcorrespondingauthor{MM}{max.mustermann@uni-tuebingen.de}
% DO USE YOUR UNIVERSITY EMAIL ADDRESS!
\icmlcorrespondingauthor{Initials1}{ansel-heng-yu.cheung@uni-tuebingen.de} 
\icmlcorrespondingauthor{Initials2}{alessio.villa@student.uni-tuebingen.de}
\icmlcorrespondingauthor{Initials3}{bartol.markovinovic@student.uni-tuebingen.de}
\icmlcorrespondingauthor{Initials4}{martin.lopez-de-ipina-munoz@student.uni-tuebingen.de}
\icmlcorrespondingauthor{Initials5}{niklas-sebastian.abraham@student.uni-tuebingen.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Cultural narratives encode and transmit evolving societal values, yet quantifying how meanings change over time remains methodologically challenging. This project investigates semantic evolution in cinema by analyzing how genres and thematic clusters shift within a unified semantic space across multiple decades. By representing movies as embeddings and tracking their geometric trajectories measuring velocity, acceleration, and curvature we distinguish periods of gradual semantic drift from moments of structural reorganization in cinematic history. This framework provides a quantitative foundation for understanding cultural change at scale and tests whether established linguistic laws of semantic evolution extend to film as a cultural medium.
\end{abstract}

\section{Introduction}\label{sec:intro}

Cinema provides a rich archive of narrative structures that encode evolving societal values across generations. Stories serve not only to entertain but to instruct, and those narratives that align with existing social values are more likely to survive and propagate through collective memory. Recent computational work has revealed hidden cultural patterns in large narrative corpora. \cite{xu2020cinderella} used word embeddings to uncover systematic gender stereotypes in movie synopses, revealing the "Cinderella complex" where female characters' happiness depends asymmetrically on male characters. \cite{matthews2021genre} applied topic modeling to investigate genre structure and temporal evolution, demonstrating that lexical features capture meaningful genre conventions and showing how genres shift in composition over time. These studies establish that quantitative methods can illuminate cultural phenomena at scales beyond traditional close reading, revealing patterns that operate across thousands of narratives.

However, measuring semantic change in cultural narratives over extended historical periods remains methodologically challenging. Previous approaches have examined genre structure at specific moments or through discrete topic models that capture lexical shifts but not the continuous geometric evolution of semantic categories. Can we characterize not merely that genres change, but how they change whether through gradual drift, sudden discontinuities, or cyclical patterns? Furthermore, while linguistic corpora have been analyzed for semantic drift using diachronic word embeddings, these methods require temporal alignment procedures that introduce potential artifacts when comparing meanings across decades.

We address these questions by constructing a unified semantic space from a large corpus of film plot summaries spanning multiple decades. By embedding all narratives into a single static vector space, we eliminate temporal alignment requirements while preserving fine grained semantic relationships. Within this space, we represent genres and thematic clusters as centroids and track their trajectories over time. By computing geometric properties of these trajectories including velocity, acceleration, and curvature we can distinguish periods of continuous semantic evolution from moments of structural reorganization where genres undergo fundamental conceptual shifts. This geometric analysis reveals not just that meanings change, but the dynamics of how they change, providing quantitative measures of cultural evolution.

\section{Related Work}\label{sec:related_work}

There have been several notable works in the area of movie plot analysis using natural language processing. (https://ceur-ws.org/Vol-3558/paper7806.pdf) makes use of SBert to create text embeddings for their movie dataset, which was also curated from Wikipedia and IMDB. They devised a "Innovation Score" which was mean cosine distance. This paper's interesting results include Awarded movies having higher IS, and that IS over time is somewhat related (28\% explained variance) to negative quadratic relationship which could be explained by how the movie space has reached saturation of ideas and any more new movie ideas would probably be similar to another movie which came out before.

(https://www.nature.com/articles/srep02758) is another relevant paper which uses crowdsourced keywords from the IMDB as a window into the contents of films and prescribe novelty scores for each film based on occurrence probabilities of individual keywords and keyword pairs. They devised a method to assign a novelty score to each film on the basis of the keywords associated with it and the keywords appearing in all films that were released prior to it. The results show that there was a surge in novel movies about the time when Hollywood went into the golden age. They also explored how novelty affected film revenue.

However little to none of the papers analyzes text embeddings from LLMs in a temporal manner.

\section{Data and Methods}\label{sec:methods}

\subsection{Data Collection}

We constructed our movie corpus using a multi stage pipeline that systematically integrated three complementary sources: Wikidata, The Movie Database (TMDb), and Wikipedia. This approach combines rich structured metadata with the detailed textual content required for semantic analysis.

Initial dataset was constructed by querying Wikidata for movies released from 1930 to 2024. In order to adhere to Wikidata's query size limitations, we iterated through the years and first acquired QIDs of all Wikidata items which have a Wikidata class that is an indirect subclass of film and have a first publication date in the given year. During this step we removed QIDs of items that do not have an English Wikipedia page associated with them. We also tried to remove non-feature movies by excluding subclasses of classes "short film" and "television series episode". However, this filtering was not perfect and further filtering of Wikidata classes was performed during post processing. After acquiring the list of identifiers, we processed them in small batches of 20 and queried Wikidata for each movie's features including title, release date, duration, genres, directors, actors, English Wikipedia link, and very importantly links to external movie databases TMDb and IMDb. Additionally, box office, box office currency, budget and budget currency values were also queried, but they had very low coverage in the raw dataset and were not used in the final analysis.

Second, we enriched the dataset using TMDb, a community driven database that offers quantitative measures of popularity and user engagement. Wikidata's external identifiers enabled direct mapping to TMDb entries, from which we programmatically retrieved vote counts, vote averages, and popularity metrics for each film. These measures served as proxies for audience engagement and cultural impact, informing downstream film filtering and weighting.

The third stage, the most data intensive, focused on obtaining full text plot summaries. Leveraging Wikipedia sitelinks from Wikidata, we accessed each film's Wikipedia page to extract the plot section. Wikipedia's editorial standards ensure relatively uniform and neutral plot descriptions, facilitating standardized comparative semantic analysis. This step used the Wikipedia API for article retrieval, section extraction, and text normalization, transforming metadata into the dense textual data required for downstream embedding.

The last enrichment stage addressed limitations in TMDb voting statistics. Many movies in the corpus lacked TMDb ratings or vote counts, and when available, these counts were often substantially lower than those reported by other platforms. To address this issue, we enriched the dataset with IMDb vote averages and vote counts, obtained from IMDb's non-commercial data files and merged using the IMDb title identifier. The inclusion of IMDb data ensures broader coverage and higher vote volumes, resulting in a more stable measure of audience reception.

All data sources are open and appropriately licensed. Wikidata \cite{wikidata} is released under CC0 1.0 Universal (public domain). Wikipedia \cite{wikipedia} is under CC BY SA 4.0, and TMDb \cite{tmdb} under CC BY NC 4.0, allowing non commercial research with attribution. This ensures reproducibility and legal compliance.

After the data was collected in a tabular format, the textual plot descriptions required transformation into vector representations via a suitable embedding model for downstream analysis. The plot descriptions extracted from Wikipedia pages exhibit substantial variability in length, ranging from 10 to 20,479 characters, corresponding to approximately 6 to 5,296 tokens in an English tokenizer. All plot descriptions in our corpus are in English, which simplifies the embedding process by eliminating cross lingual considerations. After performing the explicitly described data pipeline steps, the final dataset contained 161{,}533 data points (movies) with a average coverage of 81\% in the categories of actors, directors, genres, and year.

\subsection{Data cleaning}

After collecting the raw movie data from Wikidata, TMDb and Wikipedia, we first ensured that our dataset does not contain any duplicates with respect to Wikidata QIDs and Wikipedia links. Then we performed the following data filtration and cleaning steps:

\begin{itemize}
    \item \textbf{Filtering out movies without a Wikipedia plot.}
    \item \textbf{Removal of non feature movies.} We removed samples from our dataset that had a Wikidata class that is an indirect subclass of a class that does not describe a feature movie. Some examples of not feasible Wikidata classes include trailers, television series episodes, short films and radio programs. 
    \item \textbf{Filtering out movies with excessively long plots.} We filtered out movies with plots longer than 14,000 characters from our dataset because these plots are labeled by Wikipedia as \textit{excessively long}.
    \item \textbf{Removal of movies with low entropy plots.}
    \item \textbf{Genre filtering.} We filtered out genres that appear only once in the dataset because these genres obviously do not describe a group of movies.
    \item \textbf{Exclusion of explicit content.} We excluded movies whose primary genres fell within explicit or highly exploitative categories, such as Bavarian porn, Nazi exploitation, cannibal film, cartoon pornography, erotic film, sexploitation film, and related genres. These categories were removed in order to focus our analysis on mainstream cinematic narratives, to avoid the distorting effects that fringe, pornographic, or exploitation genres would have on cultural and semantic trends in the wider corpus, and because including them would not have been appropriate or useful for a university project of this scope.
\end{itemize}

The most critical cleaning step was the removal of movies with low-entropy plots. Raw dataset contained a significant number of incomplete or overly brief plots (e.g. \href{https://en.wikipedia.org/wiki/1982_(2013_film)}{this}). To identify and remove such movies we employed a filtering method inspired by \cite{wenzek2019ccnetextractinghighquality}, who used perplexity of a Large Language model to filter out low quality documents. While \cite{wenzek2019ccnetextractinghighquality} utilized perplexity of a 5 gram language model trained on high quality data, we tokenized the plots with the BGE-M3 tokenizer and compute the Shannon entropy of the token distribution for each plot. To determine the optimal entropy threshold, we sampled 150 movies from the borderline entropy region of $[4.0, 5.5]$ and manually annotated them as either \textit{good} or \textit{bad} quality. The threshold of $4.8398$ was chosen to maximize the $F\beta$ score with $\beta=0.5$ prioritizing precision over recall.

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/funnel_chart_data_cleaning.png}
    \caption{Data cleaning pipeline: number of movies retained after each filtering step.}
    \label{fig:data_cleaning_funnel}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/entropy_cutoff.png}
    \caption{Results of manual labeling of 150 plots in the borderline entropy region with the chosen entropy threshold}
    \label{fig:entropy_cutoff}
\end{figure}

\subsubsection{Embedding of the movie plot summaries}

The selection of an appropriate embedding model was guided by the Massive Text Embedding Benchmark (MTEB) leaderboard results\footnote{\url{https://huggingface.co/spaces/mteb/leaderboard}}, which provides comprehensive evaluations of embedding models across diverse retrieval and semantic similarity tasks. Based on these benchmarks, we selected the BGE-M3 (Beijing Academy of Artificial Intelligence Multilingual, Multifunctional, Multi granularity) model \cite{chen2024bgem3embeddingmultilingualmultifunctionality}, an open-source model developed by the Beijing Academy of Artificial Intelligence. The BGE-M3 model achieved competitive performance (28th place on the MTEB leaderboard) while maintaining a relatively compact architecture with 0.5 billion parameters. Critically, the model supports a context length of 8,192 tokens, which enables embedding entire movie plot descriptions into a single vector representation without requiring chunking.

%The BGE-M3 model provides three embedding modes: a dense vector (global document representation via [CLS]), a sparse vector (high-dimensional token weights for lexical matching), and a multi-vector mode (token-level semantic representations for each word in the input).

A key methodological choice was to use a single, static embedding model for all time periods, rather than training separate or temporally aligned models. This ensures all film plots are represented in a unified latent space, avoiding complex post hoc alignment and minimizing artifacts. While movies from earlier decades describe very different world view with a different language and culture, the Wikipedia based plot summaries are not contemporaneous texts from those eras, instead, they are modern English descriptions collectively maintained and updated since Wikipedia's founding in 2001. Thus, any linguistic variation or semantic drift in the summaries themselves is minimal. We rely on this assumption of consistent descriptive language to enable direct, meaningful comparisons of embedding based semantics across decades, without additional alignment steps.

Embedding variable length documents presents challenges for transformer based models due to fixed context windows and representational biases. Because plot summaries in our dataset span from a few sentences to thousands of words, it is crucial to select chunking and pooling strategies that minimize length bias while retaining semantic content. Relying solely on the [CLS] token for global representation can introduce substantial bias: it is sensitive to input length, often overemphasizes the first ~128 tokens \cite{DBLP:journals/corr/abs-1810-04805, pmlr-v97-gong19a}, and underperforms mean pooling on long documents \cite{raffel2023exploringlimitstransferlearning}. With over 75\% of our plots exceeding 512 tokens, a more robust aggregation method is required.

We evaluated four document embedding approaches: Mean Pooling (averaging token embeddings), CLS Token (using pretrained [CLS] token), Early Chunk-then-Embed (splitting documents before embedding), and Late Embed-then-Chunk (embedding full documents then aggregating). Each method offers different tradeoffs between bias, variance, and semantic preservation.

We systematically compared these approaches on 5,000 movie plots using four metrics: length bias (correlation between document length and embedding norm), isotropy (variance in first principal component), genre classification accuracy, and class separation (silhouette scores and separation ratios). Results showed substantial variation: length-norm correlation ranged from 0.366 to 0.822, with CLS Token achieving near zero correlation (0.004). Isotropy varied from 3.32\% to 11.92\%, while genre classification accuracy showed minimal differences (0.326 to 0.349).

Given that no single method dominated across all dimensions, we selected CLS Token based on its superior length bias mitigation (correlation of 0.004), strong isotropy properties (3.32\% first-PC variance), high separability (separation ratio of 0.958), computational efficiency, and standard usage in transformer based embeddings. While Mean Pooling and Late Chunking preserved more semantic detail, their strong length correlations (0.629 to 0.822) introduced systematic biases that could confound temporal analyses.

\subsubsection{Sanity check of the embeddings}

To validate that the embeddings capture meaningful semantic relationships, we performed a sanity check using well known movie franchises. We computed the mean embedding vector for movies belonging to three prominent franchises: Star Wars, Harry Potter, and James Bond. We then calculated two types of cosine distances: the inner group distance (between movies within the same franchise) and the group to random distance (between the franchise mean and a random sample of movies from the dataset). This analysis revealed that movies within the same franchise cluster together in the embedding space, with inner group distances significantly lower than distances to random movies. Specifically, for the Harry Potter franchise, the inner group distance was 0.210 and the group to random distance was 0.572; for Star Wars, these values were 0.349 and 0.560 respectively; and for James Bond movies, they were 0.260 and 0.527. In comparison, the global average cosine distance between two randomly selected movies was 0.520, confirming that the embeddings successfully capture semantic similarity and that movies within the same franchise are indeed closer together in the embedding space than to unrelated movies.

\subsubsection{Genre Taxonomy Construction}

The raw dataset contained 975 unique genre labels, many of which represented semantically equivalent or highly similar categories. For instance, labels such as "science fiction film" and "science fiction comedy" capture essentially the same thematic content despite their different surface forms. Additionally, the majority of movies in our corpus were associated with multiple genre labels, reflecting the hybrid nature of cinematic narratives.

To address this redundancy and create a more coherent genre taxonomy, we implemented a multi stage genre organization pipeline. First, we removed all genre labels that appeared only once in the dataset, as these singleton genres do not represent meaningful groupings of movies. This filtering step reduced the number of unique genres from 975 to 463.

Next, we sought to consolidate semantically similar genres by leveraging their textual descriptions. For each remaining genre, we retrieved its corresponding Wikipedia article using the Wikidata QID identifier. Of the 463 genres, 359 had associated Wikipedia articles containing genre descriptions. We then embedded these genre descriptions using the BGE-M3 model, creating vector representations that capture the semantic content of each genre's definition.

To identify groups of semantically similar genres, we applied k-means clustering with the Lloyd algorithm to the embedded genre descriptions. We selected $k=20$ clusters based on the balance between granularity and interpretability, resulting in 20 distinct genre categories. Each cluster was manually labeled by examining the genres it contained and identifying the common thematic elements that unified them.

The final genre taxonomy consists of 20 categories, with multi labeling allowed such that each movie can be assigned to multiple genre clusters. For example, the Biographical cluster encompasses genres such as autobiography, biographical drama film, biographical film, biography, jukebox musical, and slice of life. Similarly, the Drama cluster includes docudrama, drama, drama film, drama television series, family drama, family drama film, fantasy drama, historical drama, historical drama film, historical film, history, legal drama, medical drama, melodrama, period drama film, period film, political drama, psychological drama, psychological drama film, and war drama. This hierarchical organization enables more systematic analysis of genre evolution while preserving the nuanced multi genre nature of individual films. For all further analysis, we will use this taxonomy.

\subsection{Novelty analysis}

This part in introduction maybe:
Common public sentiment is that the film industry is "running out of ideas" resulting in movies that are becoming less creative and more similar to each other over time.

Then later:
To investigate the claim that movies are becoming less novel over time, we developed a metric for novelty defined as the minimal cosine distance between a specific movie's plot embedding and the embeddings of all movies in the dataset released prior to it. This can be formally written as:

\begin{equation}
    \text{Novelty}(m_i) = \min_{j: year_j < year_i} \left( 1 - \frac{E(m_i) \cdot E(m_j)}{\|E(m_i)\| \|E(m_j)\|} \right)
\end{equation} 

where \( E(m) \) denotes the embedding vector of a movie's plot.
Intuitively, a higher novelty score indicates that the movie's plot is more dissimilar from prior movies, while a lower score implies existence of a very similar movie released earlier.
To compiute these scores, we use the Faiss library \cite{douze2024faiss}.
Movies were sorted chronologically and processed in yearly batches. For a given batch we queried the Faiss index containing all prior movies to find distances to the nearest neighbor for each movie in the current batch. After that, the current batch was added to the index for subsequent queries.

Results:
In order to assess if temporal trends of novelty scores exist, we plot the average novelty score per year alongside scattered individual movie scores in Figure \ref{fig:novelty_over_time}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/novelty_over_time.png}
    \caption{Novelty scores of movies over time. The blue line represents the average novelty score per year, while individual movie scores are shown as scattered points.}
    \label{fig:novelty_over_time}
\end{figure}

The resulting plot indicates that the average yearly novelty remained relatively constant from 1950s onwards.
TODO: Find better way to plot this, maybe novelty of all movies vs novelty of Oscar nominees?


\subsection{Methodology}

After the data was collected and cleaned, the first step was to embed the movie plot summaries into a semantic space.

\subsubsection{Distance analysis}

Once movie plots are embedded into a unified semantic space, quantitative analysis of their geometric relationships becomes possible through distance metrics. The cosine distance between embeddings provides a natural measure of semantic dissimilarity, enabling the construction of cumulative distribution functions over pairwise distances within defined subsets of the corpus. Such distributions encode structural properties of the embedding space and reveal whether semantic relationships exhibit systematic patterns across temporal periods or thematic categories.

\subsubsection{Kolmogorov-Smirnov test}

To rigorously compare distance distributions across different subsets of movies, for instance films from different decades or belonging to distinct genres or distinct epsilon balls arround anchor movies, we employ the Kolmogorov-Smirnov test, a non parametric statistical method for assessing whether two empirical distributions arise from the same underlying continuous distribution \cite{massey1951}. The two sample KS test compares the empirical cumulative distribution functions (ECDFs) of two samples by computing the maximum vertical distance between them.

Formally, given two samples \( X_1, \ldots, X_n \) and \( Y_1, \ldots, Y_m \), their empirical cumulative distribution functions are defined as:
\begin{equation}
F_n(x) = \frac{1}{n} \sum_{i=1}^{n} 1_{X_i \leq x}, \quad G_m(y) = \frac{1}{m} \sum_{j=1}^{m} 1_{Y_j \leq y}
\end{equation}
where \( 1 \) denotes the indicator function. The KS test statistic is defined as the supremum of absolute differences between these ECDFs:
\begin{equation}
D_{n,m} = \sup_{x} |F_n(x)   G_m(x)|
\end{equation}

Under the null hypothesis that both samples are drawn from the same continuous distribution, the distribution of \( D_{n,m} \) is known and can be used to compute p values for hypothesis testing. The test is particularly suited for our application because it makes no assumptions about the underlying distributional form, is sensitive to differences in both location and shape, and operates directly on the distance measurements without requiring binning or parametric modeling.

A key design choice in applying this framework is the selection of a reference point from which distances are computed. One natural approach is to use the mean vector of a baseline subset of movies as a reference embedding, then compute the distribution of distances from this reference point to all movies in the corpus. This enables quantitative analysis of how semantic representations are spatially organized relative to fixed reference points in the embedding space.

In the context of temporal semantic analysis, the KS test enables systematic comparison of distance distributions across decades. By computing distances from fixed reference points (such as mean embeddings of genre clusters) to movies from different decades, we can assess whether the spatial organization of semantic representations evolves over time. If the semantic structure of cinema remains stable over time, distance distributions should remain statistically similar. Conversely, significant differences in these distributions, as detected by the KS test, would indicate structural reorganization of the semantic space, suggesting periods where narrative conventions undergo fundamental shifts.

\subsubsection{Epsilon Ball Construction}

To operationalize this framework, we construct epsilon balls around selected anchor movies by collecting all movies within a specified cosine distance threshold, typically $\epsilon \in [0.24, 0.30]$. Given a set of anchor movies representing a specific thematic category (e.g., spy films), all movies within the epsilon ball exhibit high plot similarity to the anchors, effectively defining a local semantic neighborhood. By comparing the distance distributions of movies within this epsilon ball to those from a control group (constructed using the mean embedding of all movies), we can quantify whether the local semantic structure differs from the global distributional properties of the corpus.

However, distance distributions alone may not fully capture temporal evolution, particularly when anchor movies belong to a series or franchise where sequels naturally cluster together. To address this limitation and analyze the temporal dimension explicitly, we construct cumulative distribution functions (CDFs) of release years for movies within the epsilon ball and compare them to the corresponding CDFs from the control group. A temporal shift in movie plots manifests as a divergence between these CDFs, indicating that the semantic neighborhood defined by the anchor movies exhibits a different temporal distribution than expected under a null model of temporal uniformity. While this approach identifies distributional differences, it does not directly test specific causal hypotheses. Interpretation of observed temporal shifts is therefore performed by examining historical context and culturally significant events within the relevant time periods.

\subsection{Genre analysis}
\input{stuff/genre_analysis.tex}

\textbf{Projection along temporal axis}
This idea is to select an arbitrary vector that is in the embedding space and project the movie embeddings onto this vector. Based on the choice of this vector, we will be able to see how much of each movie embedding lies on the vector, and we can then do temporal analysis to see how this metric evolves over time. Since our embeddings are already normalized to magnitude 1, cosine distance is proportional to L2 norm, which measures euclidean distance between embeddings. Note that for this Projection Analysis, all experiments were bootstrapped for 500 times, 1000 samples.

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/ansel/projection_analysis_action_1930_2024.png}
    \caption{Projection onto action vector over years}
    \label{fig:proj_analysis_action}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/ansel/projection_analysis_romance.png}
    \caption{Projection onto Romance vector over years}
    \label{fig:proj_analysis_romance}
\end{figure}

First vector we chose was $mean(\text{emb}_{\text{action 2024}}) - mean(\text{emb}_{\text{action 1930}})$. From Figure \ref{fig:proj_analysis_action}, we can see that movie plots are increasingly becoming similar to action movie plots. If we look at Figure \ref{fig:proj_analysis_romance} we also see the same trend for Romance. This created suspicion and we plotted the same thing but the overall centroid shift from 1930 to 2024 and also saw the same plots (Figure \ref{fig:proj_analysis_overall}). This could mean that how movie plots evolve over time outweighs the shift in genres. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/ansel/projection_analysis_overall.png}
    \caption{Projection onto Mean vector over years}
    \label{fig:proj_analysis_overall}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/ansel/action_cosine_dist.png}
    \caption{Action cosine distance to mean (all) embedding vector}
    \label{fig:action_cos_dist}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/ansel/romance_cosine_dist.png}
    \caption{Romance cosine distance to mean (all) embedding vector}
    \label{fig:romance_cos_dist}
\end{figure}

In order to then explore the shift in genres over time, we plotted the Cosine Distance evolution of mean action embedding per year from the overall centroid. From Figure \ref{fig:action_cos_dist} we observe that the action movies are getting more and more similar to the average embedding vector. The Romance cosine distance to the same mean vector shows a different story (Figure \ref{fig:romance_cos_dist}). 

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/ansel/all_cosine_dist.png}
    \caption{All genre's cosine distance to mean (all) embedding vector}
    \label{fig:all_cos_dist}
\end{figure}

Figure \ref{fig:all_cos_dist} We expanded this to our new genres and this revealed that many large genres were converging towards the mean embedding. There are some outliers such as "Anime" which only appeared after 1980. "Film Noir" and "Adventure and Fantasy" did not follow the trend of converging towards the mean embedding. (do I end here?)

\textbf{Spread analysis}
Another way to see if movies are converging over the years or spreading out is to measure the spread per year. (https://arxiv.org/pdf/1810.08693) Frobenius norm measures the total variance of each year's difference in movie embeddings to its yearly mean embedding. The frobenius norm (https://arxiv.org/pdf/1501.01571 page 84 ) is the sum of squared singular values in which we are only measuring noise and not the signal. In order to see the signal shift, we use the spectral norm to find the maximum singular value of the difference in movie embeddings and their respective yearly mean embeddings.

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/ansel/spread_analysis_mean_l2_norm.png}
    \caption{Mean L2 Norm vs Cosine Distance against yearly centroid}
    \label{fig:mean_l2_norm}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/ansel/spread_analysis_frob_spec_norm.png}
    \caption{Spectral and Frobenius Norm of yearly movies against yearly centroid}
    \label{fig:frob_spec_norm}
\end{figure}

We observe from Figure \ref{fig:mean_l2_norm} that movies are getting getting more and dissimilar. Paired together with the mean L2 distance from each movie to the yearly mean embedding, we can see that the average distance from each movie to its yearly mean remains the same, but the spectral norm triples in size (Figure \ref{fig:frob_spec_norm}), signalling that there is some sort of shape shift or stretching of the embedding cloud. There is a sharp drop in 2020, which might be attributed to movie production during covid period reducing the number of movies being produced and hence reducing the chance for outliers.

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/ansel/spread_analysis_pve.png}
    \caption{PC1 Explained Variance of yearly movies against yearly centroid}
    \label{fig:pve_yearly}
\end{figure}

The next sensible step to take is to analyze the explained variance of the first principal component. This can simply be calculated by the squared spectral norm divided by the squared frobenius norm. From Figure \ref{fig:pve_yearly}, the explained variance increases from 3\% to 4\% and sharply rises to 4.75\% after 2020. An explained variance of 4\% is significant in a dataset with dimension of 1024. If all dimensions were random noise, each PC would explain $1/1024 \approx 0.098\%$, so 4\% is 40 times higher than random. This means that there is a direction which is polarizing the movie industry. 

This is actually one large circular path leading back to PCA. Now we can interpret the evolution of movies to see which movies are the most polarized by that particular year's principal component. Hence we project every movie's embeddings onto its year's PC. From here we select an arbitrary number of years to analyze its top and bottom 5 movies.

(PCA1 table here)

(PCA US Movies)

(PCA German Movies)


\section{Results}\label{sec:results}

In this section, we present and interpret the main empirical findings of our analysis on the embedding space of movie plot summaries. Our results address the spatial structure of the embedding space, overall trends in movie similarity, and how key summary statistics illustrate broader cultural and semantic patterns.

\subsection{Kolmogorov-Smirnov test}

We begin by examining a concrete example using James Bond films as anchor movies to assess whether their distance distributions differ from those of randomly selected movies. Figure~\ref{fig:n_neighbours_bond} displays the cumulative distribution function (left panel) and histogram (right panel) of cosine distances from the anchor movies and from the mean embedding vector to all other movies in the dataset. The initial steep rise in the anchor movie CDF corresponds to other Bond films and thematically related spy movies, which exhibit minimal distances. However, these constitute a small fraction of the corpus, resulting in a CDF that remains close to zero initially before rising more gradually. In contrast, the mean vector curve lies consistently to the left of the anchor curve, indicating that the global mean embedding is more similar to the majority of movies than the highly specific Bond anchor movies. This is expected: the mean embedding represents an average over all narrative types, whereas the Bond anchor is semantically constrained to a narrow subgenre, resulting in greater distances to most films. The histogram in the right panel confirms this pattern, showing that the anchor movies exhibit a broader and more right-skewed distribution of distances compared to the mean vector.

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/n_neighbours_dist_Spectre__The_World_Is_Not_Enough__Tomorrow_Never_Dies__Casino_Royale__Casino_Royale__GoldenEye__Quantum_of_Sola__501cadb7.png}
    \caption{Distance distribution comparison for James Bond anchor movies. Left: Cumulative distribution functions of cosine distances from anchor movies (Bond films) and mean embedding vector to all movies in the dataset. Right: Corresponding histogram showing the distribution of distances. The anchor movies show greater distances to most films due to their thematic specificity.}
    \label{fig:n_neighbours_bond}
\end{figure}

As a control, we constructed an epsilon ball around a random selection of movies and compared their distance distributions to those from the mean embedding vector. Figure~\ref{fig:n_neighbours_random} demonstrates that, as expected, both distributions are nearly identical when the anchor lacks thematic coherence. This validates that observed distributional differences in the Bond example stem from genuine semantic structure rather than methodological artifacts.

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/n_neighbours_dist_All_We_Had__Joe_the_King__Redirected__Diabolically_Yours__Manjapai__Christmas_in_Wonderland__Ladybird,_Ladybird__c93721cb.png}
    \caption{Distance distribution comparison for random anchor movies. Both the anchor and mean vector distributions are nearly identical, confirming that thematically unrelated movies do not exhibit systematic distributional differences.}
    \label{fig:n_neighbours_random}
\end{figure}

We now apply the KS test framework to quantify these differences. Using an epsilon ball of radius $\epsilon = 0.30$ around the Bond anchor movies yields 159 movies, while the control group constructed from the mean embedding contains 41,018 movies. Figure~\ref{fig:ks_test_distances_bond} displays the results of the KS test on distance distributions. The left panel shows a pronounced divergence in the cumulative distribution functions, driven by the high density of semantically similar movies in close proximity to the Bond anchor. The histogram in the right panel reveals that the anchor distribution exhibits a distinct peak at lower distances, reflecting the presence of numerous spy-themed films with similar narrative structures. This local density of thematically related movies distinguishes the anchor distribution from the control group. While this result confirms that James Bond films occupy a semantically distinct region of the embedding space, it does not yet address temporal evolution.

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/ks_test_distances_Spectre__The_World_Is_Not_Enough__Tomorrow_Never_Dies__Casino_Royale__Casino_Royale__GoldenEye__Quantum_of_Sola__501cadb7_eps0.30.png}
    \caption{KS test on distance distributions for James Bond epsilon ball ($\epsilon = 0.30$, 159 movies) versus control group (41,018 movies). Left: Cumulative distribution functions showing pronounced divergence. Right: Histogram revealing high local density of semantically similar movies near the Bond anchor.}
    \label{fig:ks_test_distances_bond}
\end{figure}

To examine the temporal dimension, we construct cumulative distribution functions of release years for movies within the epsilon ball and compare them to the control group. Figure~\ref{fig:ks_test_temporal_bond} shows that the temporal distributions differ markedly. The left panel reveals a divergence beginning approximately in the 1960s, suggesting that the spy movie subgenre represented by the Bond anchor exhibits a distinct temporal emergence pattern compared to the broader corpus. The right panel displays normalized histograms of movie counts per year for both groups, confirming that the temporal distribution of spy-themed films diverges from the overall temporal distribution of cinema. Both histograms are normalized by their respective maximum yearly counts to facilitate direct comparison of temporal shapes. This temporal divergence indicates that the spy film subgenre experienced a period of increased production and thematic consolidation that is not representative of general cinematic trends during the same period.

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/ks_test_temporal_Spectre__The_World_Is_Not_Enough__Tomorrow_Never_Dies__Casino_Royale__Casino_Royale__GoldenEye__Quantum_of_Sola__501cadb7_eps0.30.png}
    \caption{KS test on temporal distributions for James Bond epsilon ball versus control group. Left: Cumulative distribution functions of release years showing divergence beginning in the 1960s. Right: Normalized histograms of movie counts per year, revealing distinct temporal patterns in spy film production compared to the broader corpus.}
    \label{fig:ks_test_temporal_bond}
\end{figure}

To further validate the methodology and explore distinct thematic categories, we applied the same framework to movies with a focus on Middle East conflicts, particularly films centered on the Gulf War and American military operations in Iraq and Afghanistan. Using anchor movies such as \textit{Black Hawk Down}, \textit{The Hurt Locker}, \textit{Zero Dark Thirty}, \textit{American Sniper}, \textit{Lone Survivor}, and \textit{13 Hours: The Secret Soldier of Benghazi}, we constructed an epsilon ball with radius $\epsilon = 0.28$. Figure~\ref{fig:ks_test_temporal_middle_east} displays the temporal distribution analysis for this thematic category. The temporal shift is even more pronounced than in the spy film case, with the largest divergence occurring prior to the Gulf War period. Following this point, the frequency of movies semantically similar to the anchor movies increases rapidly, and their temporal distribution converges more quickly toward the control group distribution. This pattern suggests that Middle East conflict films represent a temporally concentrated genre that emerged in response to specific historical events, with production rates that accelerated dramatically following the onset of these conflicts.

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/ks_test_temporal_Black_Hawk_Down__The_Hurt_Locker__Zero_Dark_Thirty__American_Sniper__Lone_Survivor__13_Hours_The_Secret_Soldier__26dca182_eps0.28.png}
    \caption{KS test on temporal distributions for Middle East conflict films epsilon ball ($\epsilon = 0.28$) versus control group. The temporal divergence is more pronounced than in the spy film case, with the largest difference occurring before the Gulf War period, followed by rapid convergence as production of conflict-themed films increased.}
    \label{fig:ks_test_temporal_middle_east}
\end{figure}

\subsection{General Spatial Analysis}

We begin with an overview of the global structure of the embedding space by examining the pairwise cosine distances between movie embeddings. Figure~\ref{fig:cosine_dist_normal_fit} displays the distribution of these cosine distances, as well as a fitted normal distribution to summarize their spread. The histogram aggregates results from three independent samples of 5000 movie pairs each, demonstrating that the relationships among movie plots in this high-dimensional space are approximately normally distributed. The mean cosine distance ($\mu = 0.5195$) and standard deviation ($\sigma = 0.0624$) characterize the typical degree of dissimilarity between movie plots. This provides a reference for understanding subsequent analyses---for example, when judging whether certain genres or time-periods are more tightly clustered or more widely dispersed than the dataset as a whole.

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/cosine_distance_normal_fit.png}
    \caption{Cosine distance distribution with fitted normal distribution. The distribution shows the pairwise cosine distances between movie embeddings, with mean $\mu = 0.5195$ and standard deviation $\sigma = 0.0624$. The histogram represents cosine distances from 3 runs with 5000 samples each, demonstrating the approximately normal structure of semantic relationships in the embedding space.}
    \label{fig:cosine_dist_normal_fit}
\end{figure}

To assess the extent to which genre labels correspond to distinct regions in the embedding space, we analyzed separation metrics across 19 genres. The overall intra-genre distance (mean cosine distance between movies within the same genre) was 0.5042, while the overall inter-genre distance (mean cosine distance between movies from different genres) was 0.5268. This yields a separation ratio of 1.0448 and a separation gap of 0.0226. The proximity of these values, with inter-genre distances only marginally exceeding intra-genre distances, indicates substantial overlap between genre clusters in the semantic space. This interpretation is further supported by a silhouette score of $-0.0334$, where negative values indicate that genres are not well separated and exhibit significant intermingling. These findings suggest that while embeddings capture semantic similarity, genre boundaries in this high dimensional space are relatively porous, reflecting the hybrid and overlapping nature of cinematic categorization.


\section{Literature and Methods Review}

\subsection{Semantic Representation and Cultural Patterns}
Our use of word embeddings to represent narrative content draws significant inspiration from the work of \textit{Xu et al. (2019)}. In their study of the "Cinderella Complex" the authors demonstrated that high-dimensional vector spaces can capture latent social biases and emotional dependencies within movie synopses. While \textit{Xu et al.} focused on the internal dynamics of characters and gender stereotypes, our research shifts the lens toward the \textbf{inter-textual relationships} between films. By embedding Wikipedia plots, we treat the narrative as a holistic semantic unit, allowing us to map the entire "cinematic universe" into a geometric space where thematic similarity is measured by vector proximity.

\subsection{Quantifying Novelty and Innovation}
To define and measure "novelty" we adopt the theoretical framework proposed by \textit{Sreenivasan (2013)}, who defines cultural innovation as the emergence of atypical combinations of elements. However, our methodology diverges from Sreenivasan's reliance on keyword frequencies and probabilistic modeling. Instead, we utilize \textbf{Geometric Distance Analysis}. In our model, novelty is operationalized as the distance of a movie vector from its $k$-nearest neighbors in the embedding space. This allows for a more nuanced detection of innovation: a film is considered "novel" if its semantic coordinates lie in a sparse region of the vector space, indicating a narrative structure that is mathematically distant from established conventions.

\subsection{Diachronic Evolution and Genre Trajectories}
The temporal dimension of our analysis is informed by the diachronic architectures discussed by \textit{Hamilton et al. (2016)}. Their research into how word meanings shift over time provides a blueprint for our analysis of genre evolution. By calculating the \textbf{centroid} of specific genres across different decades, we track their ``semantic drift.'' This approach allows us to test hypotheses regarding cultural homogenization versus diversification, a concept explored in the musical domain by \textit{Di Marco et al. (2025)}. While \textit{Di Marco et al.} utilized network science to observe the simplification of musical structures, we apply these concepts to text, observing whether movie genres are converging toward a standard ``formula'' or expanding into new, unexplored territories of the embedding world.

\section{Discussion \& Conclusion}\label{sec:conclusion}

% Use this section to briefly summarize the entire text. Highlight limitations and problems, but also make clear statements where they are possible and supported by the analysis. 

\newpage

\section*{Contribution Statement}
\textbf{Contribution Statement:}

\begin{itemize}
    \item \textbf{Ansel Cheung:} Performed genre classification analysis, classification of movie plots into genres, and conducted genre drift and PCA analysis of the movie plots.
    \item \textbf{Alessio Villa:} Developed and maintained the IMDb and TMDb API pipelines, and contributed to the related work research and methods background sections.
    \item \textbf{Bartol Markovinović:} Defined the data pipeline cutoff and carried out resulting data cleaning, managed the integration of Wikidata, and conducted novelty score analysis.
    \item \textbf{Martín López de Ipiña:} Carried out genre drift statistical analysis on the general embedding space, performed general spatial analysis of embeddings, and analyzed the cosine distance distributions.
    \item \textbf{Niklas Abraham:} Performed embedding model selection and evaluation, analyzed chunking methods, and performed KS test and distance distribution analysis.
\end{itemize}

\vspace{0.5em}
Overall, all authors contributed equally to the project. This is reflected in the various analysis sections throughout the report, where each member's work formed an integral and balanced part of the final study.


\bibliography{bibliography}
\bibliographystyle{icml2025}

\end{document}

% This document was modified from the files available at https://icml.cc/Conferences/2025/AuthorInstructions
% the full copyright notice is available within the file icml2025.sty