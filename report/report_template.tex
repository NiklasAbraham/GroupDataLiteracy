%%%%%%%% DATA LITERACY 2025 LATEX PROJECT TEMPLATE FILE %%%%%%%%%%%%%%%%%
%%% Based on the 2025 ICML template, available at https://icml.cc/Conferences/2025/AuthorInstructions %%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{tikz}
% Corporate Design of the University of Tübingen
% Primary Colors
\definecolor{TUred}{RGB}{165,30,55}
\definecolor{TUgold}{RGB}{180,160,105}
\definecolor{TUdark}{RGB}{50,65,75}
\definecolor{TUgray}{RGB}{175,179,183}

% Secondary Colors
\definecolor{TUdarkblue}{RGB}{65,90,140}
\definecolor{TUblue}{RGB}{0,105,170}
\definecolor{TUlightblue}{RGB}{80,170,200}
\definecolor{TUlightgreen}{RGB}{130,185,160}
\definecolor{TUgreen}{RGB}{125,165,75}
\definecolor{TUdarkgreen}{RGB}{50,110,30}
\definecolor{TUocre}{RGB}{200,80,60}
\definecolor{TUviolet}{RGB}{175,110,150}
\definecolor{TUmauve}{RGB}{180,160,150}
\definecolor{TUbeige}{RGB}{215,180,105}
\definecolor{TUorange}{RGB}{210,150,0}
\definecolor{TUbrown}{RGB}{145,105,70}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Project Report Template for Data Literacy 2025}

\begin{document}

\twocolumn[
\icmltitle{Plot Twists Over Time: How Movie Stories Have Changed Over 95 Years}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ansel Cheung}{equal,first}
\icmlauthor{Alessio Villa}{equal,second}
\icmlauthor{Bartol Markovinović}{equal,third}
\icmlauthor{Martín López de Ipi\~na}{equal,fourth}
\icmlauthor{Niklas Abraham}{equal,fifth}
\end{icmlauthorlist}

% fill in your matrikelnummer, email address, degree, for each group member
\icmlaffiliation{first}{Matrikelnummer 7274374, MSc Machine Learning}
\icmlaffiliation{second}{Matrikelnummer 7306912, MSc Computer Science}
\icmlaffiliation{third}{Matrikelnummer 7324790, MSc Machine Learning}
\icmlaffiliation{fourth}{Matrikelnummer 7293076, MSc Machine Learning}
\icmlaffiliation{fifth}{Matrikelnummer 7307188, MSc Machine Learning}

% put your email addresses here. You can use initials to save space, 
% e.g. if you are called Max Mustermann, you can use \icmlcorrespondingauthor{MM}{max.mustermann@uni-tuebingen.de}
% DO USE YOUR UNIVERSITY EMAIL ADDRESS!

% for the Data Literacy report, to save space, you can here list the student email address of one author, who is willing to be contacted about this work in the future (e.g. in case we would like to use your report as an example for future course iterations)
\icmlcorrespondingauthor{AC}{ansel-heng-yu.cheung@uni-tuebingen.de} 

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Cinema serves as a cultural archive that reflects evolving societal values and narrative preferences across generations. Understanding how movie stories have changed over time provides insights into cultural evolution, but quantitative analysis of narrative structures at scale has remained challenging. We address this gap by analyzing semantic evolution in cinema through embedding movie plot summaries from 1930 to 2024 into a unified semantic space. Using distance distributions, novelty scores, and Kolmogorov-Smirnov tests, we quantify temporal shifts in narrative structures across nearly a century of filmmaking. Our analysis reveals overall semantic stability in the broader corpus, suggesting that fundamental narrative patterns remain consistent over time. However, we also identify distinct emergence patterns in specific subgenres, demonstrating how thematic categories evolve and consolidate in response to cultural and historical contexts.
\end{abstract}

\section{Introduction}\label{sec:intro}

Cinema provides a rich archive of narrative structures that encode evolving societal values across generations. Previous approaches employ keyword search or topic modelling \cite{dubourg2023cinema} to explore temporal trends in movie plots. While these older methods might be insightful but challenging, recent computational work \cite{xu2020cinderella} has revealed hidden cultural patterns in large narrative corpora. One such analysis conducted on the musical domain uses high dimensional embeddings to observe changes to structural properties over time \cite{DiMarco2025}. We build upon these foundations by leveraging advances in large language models (LLMs) to embed movie plot summaries \cite{sreenivasan2013quantitative} into a unified semantic space. Using novel statistical methods, we quantitatively analyze how movie narratives evolve over time.

\section{Data and Methods}\label{sec:methods}

\subsection{Data Collection}

The initial dataset was constructed by querying Wikidata, an open knowledge graph structured as a network of items, connected together by properties. We used its SPARQL API to extract items classified as films with release dates ranging from 1930 to 2024.
% In order to adhere to Wikidata's query size limitations, we gathered the data iteratively by release year. 
We first retrieved QIDs of items describing feature films, then processed them in small batches to gather the rest of relevant features including title, release date, duration, genres, directors, actors, English Wikipedia link, and very importantly links to external movie databases TMDb and IMDb. During collection of QIDs, items classified as short film or television series episode were excluded, and final filtering of non-feature films was performed during data cleaning.

Second, we enriched the dataset with TMDb statistics, retrieving vote counts, vote averages, and popularity for each film. These served as proxies for audience engagement and informed later filtering and weighting.

The third stage, the most data-intensive, focused on obtaining full-text plot summaries. Leveraging Wikipedia sitelinks from Wikidata, we accessed each film's Wikipedia page to extract the plot section. Wikipedia's editorial standards ensure relatively uniform and neutral plot descriptions, facilitating standardized comparative semantic analysis. This step used the Wikipedia API for article retrieval, section extraction, and text normalization, transforming metadata into the dense textual data required for downstream embedding.

The last enrichment stage addressed limitations in TMDb voting statistics. Many movies in the corpus lacked TMDb ratings or vote counts, and when available, these counts were often substantially lower than those reported by other platforms. To address this issue, we enriched the dataset with IMDb vote averages and vote counts, obtained from IMDb's non-commercial data files and merged using the IMDb title identifier. The inclusion of IMDb data ensures broader coverage and higher vote volumes, resulting in a more stable measure of audience reception.

All data sources are open and appropriately licensed. Wikidata \cite{wikidata} is released under CC0 1.0 Universal (public domain). Wikipedia \cite{wikipedia} is under CC BY SA 4.0, and TMDb \cite{tmdb} under CC BY NC 4.0, allowing non-commercial research with attribution. This ensures reproducibility and legal compliance.

After data processing, our final dataset consisted of 141,119 movies with 81\% average feature coverage.

\subsection{Data cleaning}

After collecting the raw movie data from Wikidata, TMDb and Wikipedia, we first ensured that our dataset does not contain any duplicates with respect to Wikidata QIDs and Wikipedia links, and then applied several filtering steps described in Figure \ref{fig:data_cleaning_funnel}.

% \begin{itemize}
%     \item \textbf{Filtering out movies without a Wikipedia plot.}
%     \item \textbf{Removal of non-feature movies based on the Wikidata class.}
%     \item \textbf{Filtering out movies with excessively long plots.} We filtered out movies with plots longer than 14,000 characters from our dataset because these plots are labeled by Wikipedia as \textit{excessively long}.
%     \item \textbf{Removal of movies with low entropy plots.}
%     \item \textbf{Exclusion of explicit content.} We excluded movies whose primary genres fell within explicit or highly exploitative categories, such as Bavarian porn, Nazi exploitation, erotic film, and related genres. These categories were removed in order to focus our analysis on mainstream cinematic narratives and because including them would not have been appropriate or useful for a university project of this scope.
% \end{itemize}


The most critical filtering step was the removal of movies with low-entropy plots. Raw dataset contained a significant number of incomplete or overly brief plots (e.g. \href{https://en.wikipedia.org/wiki/1982_(2013_film)}{this}). To identify and remove such movies we employed a filtering method inspired by \cite{wenzek2019ccnetextractinghighquality}, who used perplexity of a Large Language model to filter out low quality documents. While \cite{wenzek2019ccnetextractinghighquality} utilized perplexity of a 5 gram language model trained on high quality data, we tokenized the plots with the BGE-M3 tokenizer and computed the Shannon entropy of the token distribution for each plot. To determine the optimal entropy threshold, we sampled 150 movies from the borderline entropy region of $[4.0, 5.5]$ and manually annotated them as either \textit{good} or \textit{bad} quality. The threshold of $4.8398$ was chosen to maximize the $F\beta$ score with $\beta=0.5$ prioritizing precision over recall.

% \begin{figure}[ht]
% \centering
% \begin{tikzpicture}[font=\small]

%     % Define Styles
%     \tikzset{
%         node_bar/.style={line width=2pt},
%         flow/.style={opacity=0.3, line join=round}
%     }

%     % --- STAGE 1: Unclean Dataset ---
%     \draw[node_bar, blue!80!black] (0, -1.5) -- (0, 1.5) node[centered, align=center, pos=0.5, black] {Unclean\\dataset\\141,119};

%     % Branch A: Movies without a plot (Exits Up)
%     \fill[flow, blue!60!gray] (0, 1.5) .. controls (1, 1.5) and (1, 2.0) .. (2, 2.0) -- (2, 1.0) .. controls (1, 1.0) and (1, 0.5) .. (0, 0.5) -- cycle;
%     \draw[node_bar, orange] (2, 1.0) -- (2, 2.0) node[centered, align=center, pos=0.5, rotate=90] {No plot\\41783};

%     % Branch B: Movies with plot (Continues Right)
%     \fill[flow, blue!60!gray] (0, 0.5) .. controls (1, 0.5) and (1, 0.8) .. (2, 0.8) -- (2, -1.5) 
%         .. controls (1, -1.5) and (1, -1.5) .. (0, -1.5) -- cycle;
%     \draw[node_bar, orange] (2, -1.5) -- (2, 0.8) node[pos=0.5, fill=white, inner sep=1pt, shift={(0.2,0)}] {99k};

%     % --- STAGE 2: Filtering Actual Movies ---
%     % Main Flow to Stage 3
%     \fill[flow, orange!60] (2, 0.8) .. controls (3, 0.8) and (3, 1.0) .. (4, 1.0) -- (4, -0.8) 
%         .. controls (3, -0.8) and (3, -1.2) .. (2, -1.2) -- cycle;
%     \draw[node_bar, red] (4, -0.8) -- (4, 1.0) node[pos=0.5, fill=white, inner sep=1pt] {98k};

%     % Small Branch Down: Wrong Class
%     \fill[flow, orange!40] (2, -1.2) .. controls (2.5, -1.2) and (3, -1.5) .. (4, -1.8) -- (4, -1.75) 
%         .. controls (3, -1.45) and (2.5, -1.5) .. (2, -1.5) -- cycle;
%     \node[right] at (4, -1.8) {Wrong class: 823};

%     % --- STAGE 3: Final Dataset ---
%     \fill[flow, purple!40] (4, 1.0) .. controls (5, 1.0) and (5, 0.8) .. (6, 0.8) -- (6, -0.6) 
%         .. controls (5, -0.6) and (5, -0.8) .. (4, -0.8) -- cycle;
%     \draw[node_bar, brown!60!black] (6, -0.6) -- (6, 0.8) node[right, black, align=left] {Final\\92,374};

% \end{tikzpicture}
\caption{Dataset curation flow (Sankey diagram style).}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/funnel_chart_data_cleaning.png}
    \caption{Data cleaning pipeline: number of movies retained after each filtering step.}
    \label{fig:data_cleaning_funnel}
\end{figure}

\textbf{Embedding of the movie plot summaries.} For embedding, we selected the BGE-M3 model \cite{chen2024bgem3embeddingmultilingualmultifunctionality} based on its strong showing (28th place) on the Massive Text Embedding Benchmark (MTEB) leaderboard\footnote{\url{https://huggingface.co/spaces/mteb/leaderboard}}. BGE-M3 is compact (0.5B parameters), supports 8,192-token context length, and can embed entire movie plots in a single vector.

We used a single static model for all time periods to ensure unified representation in a common latent space. Since Wikipedia summaries are modern English (maintained since 2001), linguistic drift is negligible. After evaluating chunking and pooling methods \cite{DBLP:journals/corr/abs-1810-04805, pmlr-v97-gong19a, raffel2023exploringlimitstransferlearning}, we selected CLS Token (details: \cite{our_github_repo}).

\textbf{Internal data validation.} To check that the embeddings meaningfully represent plot similarity, we measured cosine distances within and outside major movie franchises (Harry Potter, Star Wars, James Bond). Movies from the same franchise consistently clustered closer together (e.g., Harry Potter: 0.21 inner-group distance vs. 0.57 to random movies), while the global average distance between any two movies was 0.52. This confirms that the embeddings capture true semantic similarity.

\textbf{Genre Taxonomy.} The raw dataset included 975 unique genre labels, many of which were redundant or highly similar. To simplify and standardize the taxonomy, we first removed genres that appeared only once, reducing the set to 463. We then embedded the Wikipedia descriptions (available for 359 genres) using the BGE-M3 model, and clustered these vectors with $k=20$ using k-means. Each cluster was manually labeled based on thematic similarity, resulting in 20 coherent genre categories used in further analyses.


\section{Methodology}\label{sec:methodology}

With data collected and cleaned, we embed movie plot summaries into a semantic space and analyze the resulting embeddings using three complementary approaches: distance-based drift analysis, novelty scoring, and Kolmogorov-Smirnov tests.

\textbf{Distance analysis.} Once movie plots are embedded into a unified semantic space, quantitative analysis of their geometric relationships becomes possible through distance metrics. The cosine distance between embeddings provides a natural measure of semantic dissimilarity, enabling the construction of cumulative distribution functions over pairwise distances within defined subsets of the corpus.

As movie genres provide a meaningful taxonomy with potential temporal evolution patterns, we examine semantic drift across different time periods of an arbitrary number of years. To this end, embeddings are first grouped by genre $g$ into discrete time periods $\tau$, forming the set $\mathcal{M}_g^{(\tau)}$ of plot embeddings. For each group, two alternative representative embeddings are computed: the \textbf{centroid} (arithmetic mean) $\bar{\mathbf{e}}_g^{(\tau)}$ and the \textbf{medoid} (cosine distance minimizer embedding) $\tilde{\mathbf{e}}_g^{(\tau)}$. We employ the following metrics to analyse the drift dynamics across the groups: (i) genre drift and acceleration, and (ii) inter-genre distance.

Genre drift measures the cosine distance between representative embeddings of consecutive periods, capturing how much a genre's semantic center evolves over two time periods. Acceleration quantifies the change in drift between consecutive periods.

Inter-genre distance determines cosine distance between representatives of each pair of genres for each time period, enabling pairwise comparison between specific genres.

Since our dataset contains significantly more movies in later years, the medoid and centroid embedding estimators are heavily affected by sampling variance. To mitigate this, we downsample to ensure equal sampling error across all genre-year groups and apply bootstrapping to estimate confidence intervals. 

\textbf{Novelty analysis.} To investigate the claim that movies are becoming less novel over time, we propose a novelty metric defined as the minimal cosine distance between a specific movie's plot embedding and the embeddings of all movies in the dataset released prior to it. This can be formally written as:

\vspace{-1.5em}
\begin{equation}
\label{novelty}
    \text{Novelty}(m_i) = \min_{j: \text{year}_j < \text{year}_i} \left( 1 - \frac{emb_i \cdot emb_j}{\|emb_i\| \|emb_j\|} \right)
\end{equation}

where \( emb_i \) denotes the embedding vector of $i$-th movie's plot.
Intuitively, a higher novelty score indicates that the movie's plot is more dissimilar from prior movies, while a lower score implies existence of a very similar movie released earlier.
To compute these scores, we leverage the Faiss library \cite{douze2024faiss} to efficiently find the preceding nearest neighbors of movie plot embeddings based on the cosine distance.

\textbf{Kolmogorov-Smirnov test.} To compare distance distributions across movie subsets (e.g., different decades or genres), we employ the Kolmogorov-Smirnov test \cite{massey1951}. We construct $\epsilon$-balls ($\epsilon \in [0.24, 0.30]$) around anchor movies representing specific themes, collecting all movies within the distance threshold to define local semantic neighborhoods. We compare distance distributions and temporal CDFs (cumulative distribution functions of release years) within these neighborhoods to a control group (global mean embedding). Divergence indicates temporal shifts: if semantic structure remains stable, distributions should be similar across time periods.

\section{Results}\label{sec:results}

In this section, we present and interpret the main empirical findings of our analysis on the embedding space of movie plot summaries.

\subsection{General Spatial Analysis}

We begin by examining the global structure of the embedding space through pairwise cosine distances between movies. These distances are approximately normally distributed, with a mean of $\mu = 0.5195$ and a standard deviation of $\sigma = 0.0624$, capturing the typical dissimilarity between movie plots and serving as a baseline for further analysis.

To evaluate whether genre labels map to distinct regions, we compared intra-genre (mean: 0.5042) and inter-genre (mean: 0.5268) distances across 19 genres. The resulting separation gap of 0.0226, ratio of 1.0448, and a silhouette score of $-0.0334$ indicate considerable overlap, with genres only weakly separated in the embedding space. This suggests genre boundaries are porous, consistent with the hybrid and overlapping nature of film categories.


\subsection{Genre drift analysis}

% I could add the plot but it doesn't show shit
After normalization, the cosine distance of each time group representative embedding with respect to the previous one exhibited only random fluctuation with no clear trend; inter-genre analysis yielded the same result. These outcomes suggest that genres may be too broad as analytical categories, with any underlying patterns likely obscured by noise. 

\subsection{Spread analysis}

We analyzed the spread of movies per year using three metrics: mean L2 norm, Frobenius norm, and spectral norm of each movie embedding relative to its yearly centroid. All metrics were computed on centered yearly embeddings (i.e. yearly centroid was subtracted from each movie embedding before computing the norms) \cite{yamagiwa2024norm}. As with the drift analysis, no interpretable patterns emerged: mean L2 norm and Frobenius norm stayed relatively constant at 0.7 and 12.4 respectively, whereas spectral norm had a slight increase from 2.1 to 2.8. This indicates that the overall spread of movies remains relatively constant over the years with outliers becoming more polarizing. The nature of these polarizing axes proved difficult to characterize, as they changed yearly and were a combination of multiple dimensions.

\subsection{Novelty score}

In order to judge whether movies are becoming less novel over time, we computed novelty scores as defined in \ref{novelty}, and plotted them against release years in Figure~\ref{fig:novelty_over_time}.

\begin{figure}[ht]
    \centering
    \includegraphics{figures/oscar_novelty_scatter.pdf}
    \caption{Novelty scores of all movies (blue) and Best Original Screenplay Oscar nominees (orange) over time. Each dot represents a movie and the lines represent yearly average novelty scores.}
    \label{fig:novelty_over_time}
\end{figure}

Fitting a linear regression to the average yearly novelty scores of all movies results in a slope of $3.3 \cdot 10^{-4}$ and $R^2$ value of $0.56$, indicating that the yearly averages have stayed almost constant over time. Additionally, we compared these averages to the yearly averages of Best Original Screenplay Oscar nominees by computing the Mean Absolute Error (MAE) between the two time series. The resulting MAE of $0.0098 \pm 0.008$ suggests that the scores of nominees do not significantly differ from the yearly novelty averages of all movies.

\subsection{Kolmogorov-Smirnov test}

We illustrate our approach using James Bond films as anchor movies to compare their distance distributions against all other movies. The cumulative distribution of cosine distances from Bond anchors rises steeply only for a small set of closely related films, while most movies remain more distant. This contrasts with the global mean embedding, which is closer on average to all movies, as it represents an average narrative rather than a specific subgenre.

To examine the temporal dimension, we constructed cumulative distribution functions of release years for movies within the epsilon ball and compared them to the control group. Figure~\ref{fig:ks_test_temporal_bond} shows that the temporal distributions differ markedly. The left panel reveals a divergence beginning approximately in the 1960s, suggesting that the spy movie subgenre represented by the Bond anchor exhibits a distinct temporal emergence pattern compared to the broader corpus. The right panel displays normalized histograms of movie counts per year for both groups, confirming that the temporal distribution of spy-themed films diverges from the overall temporal distribution of cinema. This temporal divergence indicates that the spy film subgenre experienced a period of increased production and thematic consolidation that is not representative of general cinematic trends during the same period.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{../figures_final/epsilon_ball_analysis/ks_test_temporal_Spectre__The_World_Is_Not_Enough__Tomorrow_Never_Dies__Casino_Royale__Casino_Royale__GoldenEye__Quantum_of_Sola__501cadb7_eps0.29.pdf}
    \vspace{-2em}
    \caption{KS test of temporal distributions for James Bond epsilon ball ($\varepsilon = 0.29$) versus control group, showing temporal divergence. Left: Cumulative distribution functions. Right: Normalized yearly counts.}
    \label{fig:ks_test_temporal_bond}
\end{figure}

    % To further validate the methodology, we applied the same framework to movies focused on Middle East conflicts, using anchor movies such as \textit{Black Hawk Down}, \textit{The Hurt Locker}, \textit{Zero Dark Thirty}, and \textit{American Sniper}. Figure~\ref{fig:ks_test_temporal_middle_east} displays the temporal distribution analysis for this thematic category. The temporal shift is even more pronounced than in the spy film case, with the largest divergence occurring prior to the Gulf War period. Following this point, the frequency of movies semantically similar to the anchor movies increases rapidly. This pattern suggests that Middle East conflict films represent a temporally concentrated genre that emerged in response to specific historical events.

% \begin{figure}[ht]
%    \centering
%    \includegraphics[width=8cm]{../figures_final/epsilon_ball_analysis/ks_test_temporal_Black_Hawk_Down__The_Hurt_Locker__Zero_Dark_Thirty__American_Sniper__Lone_Survivor__13_Hours_The_Secret_Soldier__26dca182_eps0.28.png}
%    \caption{KS test on temporal distributions for Middle East conflict films epsilon ball ($\epsilon = 0.28$) versus control group. The temporal divergence is more pronounced than in the spy film case, with the largest difference occurring before the Gulf War period, followed by rapid convergence as production of conflict-themed films increased.}
%    \label{fig:ks_test_temporal_middle_east}
% \end{figure}

\section{Discussion \& Conclusion}\label{sec:conclusion}
We have utilized modern embedding methods and multiple statistical tools to analyze the evolution of movie plot embeddings over nearly a century of cinema. Our findings indicate that while the overall semantic structure of movie plots remains relatively stable, specific thematic subgenres exhibit distinct temporal emergence patterns. 

We must acknowledge the limitations that arise from our data sources. Wikipedia plot summaries, while standardized, may not fully capture the nuances of original narratives, potentially introducing bias. Additionally, our reliance on a single embedding model, while ensuring a unified semantic space, may overlook temporal linguistic shifts. More importantly, the evolution of cinema is not only reflected in plot summaries but also in cinematography, direction, music, feeling, acting and other non-textual elements. Future work could explore multimodal embeddings that integrate visual and auditory features alongside textual data.

All code and data used in this project are openly available in our GitHub repository\cite{our_github_repo} and as a dedicated dataset on Hugging Face\cite{our_data_on_huggingface}.

\clearpage

\section*{Contribution Statement}
\textbf{Contribution Statement:}

\begin{itemize}
    \item \textbf{Ansel Cheung:} Performed genre classification analysis, classification of movie plots into genres, and conducted genre drift and PCA analysis of the movie plots.
    \item \textbf{Alessio Villa:} Developed and maintained the IMDb and TMDb API pipelines, and contributed to the related work research and methods background sections.
    \item \textbf{Bartol Markovinović:} Defined the data pipeline cutoff and carried out resulting data cleaning, managed the integration of Wikidata, and conducted novelty score analysis.
    \item \textbf{Martín López de Ipiña:} Carried out genre drift statistical analysis on the general embedding space, performed general spatial analysis of embeddings, and analyzed the cosine distance distributions.
    \item \textbf{Niklas Abraham:} Performed embedding model selection and evaluation, analyzed chunking methods, and performed KS test and distance distribution analysis.
\end{itemize}

\vspace{0.5em}
Overall, all authors contributed equally to the project. This is reflected in the various analysis sections throughout the report, where each member's work formed an integral and balanced part of the final study.


\bibliography{bibliography}
\bibliographystyle{icml2025}
    
\end{document}

% This document was modified from the files available at https://icml.cc/Conferences/2025/AuthorInstructions
% the full copyright notice is available within the file icml2025.sty