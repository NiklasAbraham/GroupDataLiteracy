%%%%%%%% DATA LITERACY 2025 LATEX PROJECT TEMPLATE FILE %%%%%%%%%%%%%%%%%
%%% Based on the 2025 ICML template, available at https://icml.cc/Conferences/2025/AuthorInstructions %%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{makecell} % for multi-line table cells

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, calc, fit}
% Corporate Design of the University of Tübingen
% Primary Colors
\definecolor{TUred}{RGB}{165,30,55}
\definecolor{TUgold}{RGB}{180,160,105}
\definecolor{TUdark}{RGB}{50,65,75}
\definecolor{TUgray}{RGB}{175,179,183}

% Secondary Colors
\definecolor{TUdarkblue}{RGB}{65,90,140}
\definecolor{TUblue}{RGB}{0,105,170}
\definecolor{TUlightblue}{RGB}{80,170,200}
\definecolor{TUlightgreen}{RGB}{130,185,160}
\definecolor{TUgreen}{RGB}{125,165,75}
\definecolor{TUdarkgreen}{RGB}{50,110,30}
\definecolor{TUocre}{RGB}{200,80,60}
\definecolor{TUviolet}{RGB}{175,110,150}
\definecolor{TUmauve}{RGB}{180,160,150}
\definecolor{TUbeige}{RGB}{215,180,105}
\definecolor{TUorange}{RGB}{210,150,0}
\definecolor{TUbrown}{RGB}{145,105,70}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Project Report Template for Data Literacy 2025}

\begin{document}

\twocolumn[
% \icmltitle{The Geometry of Cinema: Quantifying 75 Years of Cultural Drift}
\icmltitle{Data Literacy 2025 Project Report}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Ansel Cheung}{equal,first}
\icmlauthor{Alessio Villa}{equal,second}
\icmlauthor{Bartol Markovinović}{equal,third}
\icmlauthor{Martín López de Ipi\~na Mu\~noz}{equal,fourth}
\icmlauthor{Niklas Abraham}{equal,fifth}
\end{icmlauthorlist}

% fill in your matrikelnummer, email address, degree, for each group member
\icmlaffiliation{first}{Matrikelnummer 7274374, MSc Machine Learning}
\icmlaffiliation{second}{Matrikelnummer 7306912, MSc Computer Science}
\icmlaffiliation{third}{Matrikelnummer 7324790, MSc Machine Learning}
\icmlaffiliation{fourth}{Matrikelnummer 7293076, MSc Machine Learning}
\icmlaffiliation{fifth}{Matrikelnummer 7307188, MSc Machine Learning}

% put your email addresses here. You can use initials to save space, 
% e.g. if you are called Max Mustermann, you can use \icmlcorrespondingauthor{MM}{max.mustermann@uni-tuebingen.de}
% DO USE YOUR UNIVERSITY EMAIL ADDRESS!
\icmlcorrespondingauthor{Initials1}{ansel-heng-yu.cheung@uni-tuebingen.de} 
\icmlcorrespondingauthor{Initials2}{alessio.villa@student.uni-tuebingen.de}
\icmlcorrespondingauthor{Initials3}{bartol.markovinovic@student.uni-tuebingen.de}
\icmlcorrespondingauthor{Initials4}{martin.lopez-de-ipina-munoz@student.uni-tuebingen.de}
\icmlcorrespondingauthor{Initials5}{niklas-sebastian.abraham@student.uni-tuebingen.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
This project addresses the fundamental question of how cultural meaning evolves over time by quantitatively modeling seventy-five years of cinematic history through 200,000 film synopses embedded in a single static semantic space using the BGE-M3 model. Temporal change is measured by tracking the movement of genre centroids within this space—analyzing their velocity, acceleration, and curvature to distinguish continuous evolution from structural paradigm shifts. The framework provides a reproducible and data-driven foundation for cultural analytics, testing whether established linguistic laws of semantic drift extend to the domain of cinema.
\end{abstract}

\section{Introduction}\label{sec:intro}

Motivate the problem, situation or topic you decided to work on. Describe why it matters (is it of societal, economic, scientific value?). Outline the rest of the paper (use references, e.g.~to \Cref{sec:methods}: What kind of data you are working with, how you analyse it, and what kind of conclusion you reached. The point of the introduction is to make the reader want to read the rest of the paper.)

\section{Data and Methods}\label{sec:methods}

Data collection process, bla bla bla, the pipline explained, etc.

\begin{table}[ht]
    \centering
    \caption{Per-decade feature coverage (\%) of key metadata fields in the movie dataset.}
    \begin{tabular}{lcccc}
        \toprule
        Decade & Actors+Director  & Genre & Plot & Vote Count \\
        \midrule
        1950s   & 86.56  & 63.67 & 83.03 & 90.19 \\
        1960s   & 83.82  & 61.01 & 77.31 & 85.26 \\
        1970s   & 86.55  & 62.47 & 79.57 & 86.58 \\
        1980s   & 84.68  & 59.41 & 79.80 & 85.41 \\
        1990s   & 82.22  & 58.33 & 82.00 & 84.49 \\
        2000s   & 77.34  & 60.51 & 84.19 & 83.73 \\
        2010s   & 70.25  & 60.55 & 84.94 & 85.60 \\
        2020s   & 70.66  & 64.85 & 77.59 & 89.63 \\
        \midrule
        Average & 80.26  & 61.35 & 81.05 & 86.36 \\
        \bottomrule
    \end{tabular}
    \label{tab:decade_coverage}
\end{table}



After the data was collected in a tabular format, the textual plot descriptions required transformation into vector representations suitable for downstream analysis. The plot descriptions extracted from Wikipedia pages exhibit substantial variability in length, ranging from 10 to 20,479 characters, corresponding to approximately 6 to 5,296 tokens. All plot descriptions in our corpus are in English, which simplifies the embedding process by eliminating cross-lingual considerations.

The selection of an appropriate embedding model was guided by the Massive Text Embedding Benchmark (MTEB) leaderboard results\footnote{\url{https://huggingface.co/spaces/mteb/leaderboard}}, which provides comprehensive evaluations of embedding models across diverse retrieval and semantic similarity tasks. Based on these benchmarks, we selected the BGE-M3 (Beijing Academy of Artificial Intelligence Multilingual, Multifunctional, Multi-granularity) model \cite{chen2024bgem3embeddingmultilingualmultifunctionality}, an open-source model developed by the Beijing Academy of Artificial Intelligence. The BGE-M3 model achieved competitive performance (28th place on the MTEB leaderboard) while maintaining a relatively compact architecture with 0.5 billion parameters. Critically, the model supports a context length of 8,192 tokens, which enables embedding entire movie plot descriptions into a single vector representation without requiring chunking for the majority of documents in our corpus.

The BGE-M3 model offers three distinct embedding modes, each suited for different retrieval and analysis tasks. The \textit{dense vector} output corresponds to the [CLS] token representation from the final transformer layer, producing a 1024-dimensional vector that serves as a global document representation \cite{DBLP:journals/corr/abs-1810-04805}. The \textit{sparse vector} mode generates token-level weights with extremely high dimensionality (250,002 dimensions), where each token is represented by a single weight, enabling fine-grained lexical matching. The \textit{multi-vector} output provides all hidden states from the model, yielding 1024-dimensional vectors for each token in the input sequence, thus preserving token-level semantic information.

A fundamental challenge in embedding variable-length documents arises from the inherent limitations of transformer-based models. Our corpus contains documents ranging from a few sentences to several thousand words, while transformer models exhibit fixed token limits and demonstrate biases in their learned representations. Specifically, the [CLS] token, which is commonly used as a document-level representation, has been shown to exhibit length-dependent biases and structural preferences \cite{DBLP:journals/corr/abs-1810-04805}. Therefore, selecting an appropriate pooling and chunking strategy is essential to minimize length bias while preserving semantic fidelity across documents of varying lengths.

While the [CLS] token approach appears attractive due to its simplicity and the model's pre-training objective of learning document-level representations, empirical evidence demonstrates significant limitations for long documents. \cite{pmlr-v97-gong19a} showed that attention entropy at the [CLS] token drops sharply as sequence length increases, with the token focusing predominantly on the first approximately 128 tokens. Similarly, \cite{raffel2023exploringlimitstransferlearning} reported that learned summary tokens underperform mean pooling for long documents, with decoder attention mechanisms favoring earlier positions in the sequence. Given that over 75\% of the plot descriptions in our corpus exceed 512 tokens, relying solely on the [CLS] token would result in substantial information loss for the majority of documents.

We considered four primary approaches for generating document embeddings, visualised in Figure \ref{fig:chunking_methods}:

\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{figures/data_preprocess/plot_different_chunking.png}
    \caption{Comparison of different chunking methods: (a) Mean Pooling, (b) First Chunk then Embed, and (c) Late Chunking.}
    \label{fig:chunking_methods}
\end{figure}

\textbf{Mean Pooling (Global Average):} This classical approach computes the mean of all token embeddings to produce a single document representation. While this method exhibits low variance, it introduces high bias by treating all tokens equally, potentially diluting important semantic information.

\textbf{Chunk-then-Embed (Early Chunking):} This strategy splits long documents into smaller chunks before embedding, processes each chunk separately, and then aggregates the resulting embeddings. This approach, exemplified by hierarchical attention networks \cite{yang-etal-2016-hierarchical}, can reduce bias but introduces higher variance due to the loss of cross-chunk contextual information.

\textbf{Embed-then-Chunk (Late Chunking):} This method, recently proposed by \cite{günther2025latechunkingcontextualchunk}, embeds the full text in a single forward pass through the transformer model, then pools token embeddings over fixed-size windows after contextualization. This approach maintains full contextual information while providing controlled variance and minimal bias.

\textbf{CLS Token Extraction:} The simplest approach utilizes only the [CLS] token from the final layer as the document representation, relying on the model's learned summarization capabilities.

To determine the optimal chunking and pooling strategy for our specific use case, we conducted comparative tests across multiple approaches.








\section{Results}\label{sec:results}

In this section outline your results. At this point, you are just stating the outcome of your analysis. You can highlight important aspects (``we observe a significantly higher value of $x$ over $y$''), but leave interpretation and opinion to the next section. This section absoultely \emph{must} include at least two figures.

\section{Discussion \& Conclusion}\label{sec:conclusion}

Use this section to briefly summarize the entire text. Highlight limitations and problems, but also make clear statements where they are possible and supported by the analysis. 

\newpage

\section*{Contribution Statement}
Explain here, in one sentence per person, what each group member contributed. For example, you could write: Max Mustermann collected and prepared data. Gabi Musterfrau and John Doe performed the data analysis. Jane Doe produced visualizations. All authors will jointly wrote the text of the report. Note that you, as a group, a collectively responsible for the report. Your contributions should be roughly equal in amount and difficulty.

\section*{Notes} 

Your entire report has a \textbf{hard page limit of 4 pages} excluding references and the contribution statement. (I.e. any pages beyond page 4 must only contain the contribution statement and references). Appendices are \emph{not} possible. But you can put additional material, like interactive visualizations or videos, on a githunb repo (use \href{https://github.com/pnkraemer/tueplots}{links} in your pdf to refer to them). Each report has to contain \textbf{at least three plots or visualizations}, and \textbf{cite at least two references}. More details about how to prepare the report, inclucing how to produce plots, cite correctly, and how to ideally structure your github repo, will be discussed in the lecture, where a rubric for the evaluation will also be provided.


\bibliography{bibliography}
\bibliographystyle{icml2025}

\end{document}

% This document was modified from the files available at https://icml.cc/Conferences/2025/AuthorInstructions
% the full copyright notice is available within the file icml2025.sty